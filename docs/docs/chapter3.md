---
sidebar_position: 3
title: "Chapter 3: Error Management Design Patterns"
description: "Design patterns for handling unprocessable records, duplicates, late data, and fault tolerance in data pipelines"
---

import { Box, Arrow, Row, Column, Group, DiagramContainer, ProcessFlow, TreeDiagram, CardGrid, StackDiagram, ComparisonTable, colors } from '@site/src/components/diagrams';
import PythonRunner from '@site/src/components/PythonRunner';

# Chapter 3: Error Management Design Patterns

> **"In this world, nothing can be said to be certain, except errors and data quality issues."**
>
> â€” Adapted from Benjamin Franklin

---

## Table of Contents

1. [Introduction](#1-introduction)
2. [Unprocessable Records](#2-unprocessable-records)
   - 2.1. [Pattern: Dead-Letter](#21-pattern-dead-letter)
3. [Duplicated Records](#3-duplicated-records)
   - 3.1. [Pattern: Windowed Deduplicator](#31-pattern-windowed-deduplicator)
4. [Late Data](#4-late-data)
   - 4.1. [Pattern: Late Data Detector](#41-pattern-late-data-detector)
   - 4.2. [Pattern: Static Late Data Integrator](#42-pattern-static-late-data-integrator)
   - 4.3. [Pattern: Dynamic Late Data Integrator](#43-pattern-dynamic-late-data-integrator)
5. [Filtering](#5-filtering)
   - 5.1. [Pattern: Filter Interceptor](#51-pattern-filter-interceptor)
6. [Fault Tolerance](#6-fault-tolerance)
   - 6.1. [Pattern: Checkpointer](#61-pattern-checkpointer)
7. [Summary](#7-summary)

---

## 1. Introduction

**In plain English:** Your data pipelines will encounter errorsâ€”it's not a matter of "if" but "when." These patterns help you handle the inevitable gracefully.

**In technical terms:** Error management design patterns provide systematic approaches to handling unprocessable records, duplicates, late-arriving data, and system failures in data engineering workflows.

**Why it matters:** Without proper error handling, a single bad record can crash your entire pipeline, leaving downstream consumers without data and creating operational nightmares.

Data is dynamic, and your expectations from today won't remain the same throughout the lifecycle. You're processing data generated by others, directly inheriting their issuesâ€”unreliable networks causing late delivery, temporary crashes causing retried deliveries, and subsequent duplicates.

<CardGrid
  columns={3}
  cards={[
    { title: "Data Quality", icon: "ðŸ“Š", color: colors.red, items: ["Unprocessable records", "Duplicates", "Late arrivals"] },
    { title: "System Failures", icon: "âš ï¸", color: colors.orange, items: ["Hardware issues", "Network problems", "Job crashes"] },
    { title: "Human Errors", icon: "ðŸ‘¤", color: colors.purple, items: ["Buggy filters", "Wrong configurations", "Missing data"] }
  ]}
/>

---

## 2. Unprocessable Records

Data quality is a recurrent problem in data projects. Unprocessable records often cause fatal failures that stop data processing jobs. However, maintaining a fail-fast approach isn't always possible, especially for long-running streaming jobs.

> **Insight**
>
> **Transient vs. Nontransient Errors:** Transient errors are temporary and will recover automatically (like brief database unavailability). Nontransient errorsâ€”also known as "poison pill" messagesâ€”are fatal issues requiring manual intervention.

### 2.1. Pattern: Dead-Letter

**In plain English:** Instead of crashing when you hit a bad record, set it aside for later investigation and keep processing the good onesâ€”like a postal service returning undeliverable mail to a "dead letter" office.

**In technical terms:** The Dead-Letter pattern isolates unprocessable records to a separate storage location while allowing the main pipeline to continue processing valid data.

**Why it matters:** For long-running streaming jobs, you can't afford to stop the entire pipeline because of one malformed record. This pattern keeps your data flowing while preserving problematic records for debugging.

#### Problem

Your stream processing job writes visit events from Apache Kafka to an object store. Recently, data producers started generating unprocessable records, causing job failures. You've spent three consecutive days manually relaunching the job and altering checkpoint offsets. You need a better solution that keeps the pipeline running and allows error investigation later.

#### Solution

<DiagramContainer title="Dead-Letter Pattern Architecture">
  <Column gap="lg">
    <Row gap="md" align="center">
      <Box color={colors.blue} icon="ðŸ“¥">Data Source</Box>
      <Arrow direction="right" />
      <Box color={colors.purple} icon="âš™ï¸">Processing Job</Box>
    </Row>
    <Row gap="lg" align="center">
      <Column gap="sm" align="center">
        <Arrow direction="down" label="valid" />
        <Box color={colors.green} icon="âœ…">Main Output</Box>
      </Column>
      <Column gap="sm" align="center">
        <Arrow direction="down" label="errors" />
        <Box color={colors.red} icon="âŒ">Dead-Letter Store</Box>
      </Column>
    </Row>
    <Row gap="md" align="center">
      <Box color={colors.slate} icon="ðŸ“Š">Monitoring</Box>
      <Arrow direction="right" label="optional" />
      <Box color={colors.orange} icon="ðŸ”„">Replay Pipeline</Box>
    </Row>
  </Column>
</DiagramContainer>

The implementation involves:

1. **Identify failure points** in your code (mapping functions, transformations)
2. **Add safety controls** (try-catch blocks, if-else conditions)
3. **Configure separate output** for erroneous events
4. **Add metadata** using the Metadata Decorator pattern for post-analysis

When choosing dead-letter storage, consider:

| Factor | Consideration |
|--------|---------------|
| **Resiliency** | Don't need a dead-letter strategy for your dead-letter storage |
| **Monitoring** | Key success factorâ€”know when errors spike |
| **Performance** | Writing unprocessed records incurs execution cost |

Good candidates include cloud object stores and streaming brokersâ€”highly available, fast, and easy to monitor.

#### Example: Apache Flink Dead-Letter

```python
# Side output declaration
invalid_data_output: OutputTag = OutputTag('invalid_visits', Types.STRING())
visits: DataStream = data_source.map(
    MapJsonToReducedVisit(invalid_data_output),
    Types.STRING()
)

# Mapping function with error handling
def map_rows(self, json_payload: str) -> str:
    try:
        evt = json.loads(json_payload)
        evt_time = int(datetime.datetime.fromisoformat(evt['event_time']))
        yield json.dumps({
            'visit_id': evt['visit_id'],
            'event_time': evt_time,
            'page': evt['page']
        })
    except Exception as e:
        yield self.invalid_data_output, _wrap_input_with_error(json_payload, e)

# Sink configuration
visits.get_side_output(invalid_data_output).sink_to(kafka_sink_invalid_data)
visits.sink_to(kafka_sink_valid_data)
```

#### Example: Apache Spark with Error-Safe Functions

Error-safe functions like `CONCAT` return NULL instead of throwing exceptions:

```sql
SELECT type, full_name, version, name_with_version,
  CASE
    WHEN (full_name IS NOT NULL OR version IS NOT NULL)
      AND name_with_version IS NULL THEN false
    ELSE true
  END AS is_valid
FROM (
  SELECT type, full_name, version,
         CONCAT(full_name, version) AS name_with_version
  FROM devices_to_load
)
```

```python
# Persist to avoid double execution
devices_to_load_with_validity_flag.persist()

# Write valid records
(devices_to_load_with_validity_flag
    .filter('is_valid IS TRUE')
    .drop('is_valid')
    .write.mode('overwrite')
    .format('delta').save(f'{base_dir}/output/devices-table'))

# Write invalid records to dead-letter
(devices_to_load_with_validity_flag
    .filter('is_valid IS FALSE')
    .drop('is_valid')
    .write.mode('overwrite')
    .format('delta').save(f'{base_dir}/output/devices-dead-letter-table'))
```

#### Consequences

<DiagramContainer title="Snowball Backfilling Effect">
  <Column gap="md">
    <Row gap="md" align="center">
      <Box color={colors.red} icon="ðŸ”„">Pipeline 1 Backfill</Box>
    </Row>
    <Arrow direction="down" label="triggers" />
    <Row gap="md" align="center">
      <Box color={colors.orange} icon="ðŸ”„">Consumer A Backfill</Box>
      <Box color={colors.orange} icon="ðŸ”„">Consumer B Backfill</Box>
    </Row>
    <Arrow direction="down" label="triggers" />
    <Row gap="md" align="center">
      <Box color={colors.purple} icon="ðŸ”„">Consumer C</Box>
      <Box color={colors.purple} icon="ðŸ”„">Consumer D</Box>
      <Box color={colors.purple} icon="ðŸ”„">Consumer E</Box>
    </Row>
  </Column>
</DiagramContainer>

> **Warning**
>
> **Snowball Backfilling:** When you replay dead-lettered records, downstream consumers may need to reprocess data, triggering a cascade of backfills throughout your system.

| Consequence | Description |
|-------------|-------------|
| **Snowball backfilling** | Replaying records can trigger cascading backfills in downstream consumers |
| **Record identification** | Consider adding a `was_dead_lettered` column to track replayed records |
| **Ordering inconsistency** | Replayed records may break ordering guarantees |
| **Hidden failures** | Pattern can mask fatal issuesâ€”complete with alerting for high error rates |

---

## 3. Duplicated Records

Exactly-once delivery is challenging in distributed systems. More often, you'll work with at-least-once delivery where records can arrive multiple times.

### 3.1. Pattern: Windowed Deduplicator

**In plain English:** Like a bouncer with a guest list who remembers who's already entered the partyâ€”within a certain time window, they won't let the same person in twice.

**In technical terms:** The Windowed Deduplicator pattern eliminates duplicate records by tracking unique identifiers within a defined scopeâ€”the entire dataset for batch jobs, or time-based windows for streaming jobs.

**Why it matters:** Duplicated data leads to incorrect aggregations, inflated metrics, and misleading business decisions. Processing each occurrence only once ensures data accuracy.

> **Insight**
>
> **Automatic Retries Trade-off:** Exactly-once processing only works without runtime errors. Restarted jobs may reprocess already-processed records despite deduplication logic. This is an accepted trade-off between automated transient error management and deduplication.

#### Problem

Your batch job processes visit events synchronized from a streaming layer. The job must guarantee exactly-once processing because business users consume the data directly. However, the streaming layer often has duplicates due to automatic retries from data producers.

#### Solution

<DiagramContainer title="Windowed Deduplicator for Streaming">
  <Row gap="md" align="center">
    <Box color={colors.blue} icon="ðŸ“¥">Input Record</Box>
    <Arrow direction="right" />
    <Box color={colors.purple} icon="ðŸ”‘">Extract Key</Box>
    <Arrow direction="right" />
    <Box color={colors.orange} icon="ðŸ’¾">Check State Store</Box>
  </Row>
</DiagramContainer>

<DiagramContainer title="Deduplication Decision Flow">
  <Column gap="md">
    <Row gap="lg" align="center">
      <Group title="Key Already Seen?" color={colors.orange}>
        <Row gap="md">
          <Column gap="sm" align="center">
            <Box color={colors.green} icon="âœ…" size="sm">No</Box>
            <Arrow direction="down" />
            <Box color={colors.green} icon="ðŸ“¤" size="sm">Process & Store Key</Box>
          </Column>
          <Column gap="sm" align="center">
            <Box color={colors.red} icon="âŒ" size="sm">Yes</Box>
            <Arrow direction="down" />
            <Box color={colors.slate} icon="ðŸ—‘ï¸" size="sm">Drop Record</Box>
          </Column>
        </Row>
      </Group>
    </Row>
  </Column>
</DiagramContainer>

**Implementation approaches:**

| Mode | Approach |
|------|----------|
| **Batch** | `DISTINCT` expression or `ROW_NUMBER()` window function |
| **Streaming** | State store to track processed keys within time windows |

**State Store Types:**

<CardGrid
  columns={3}
  cards={[
    { title: "Local", icon: "ðŸ’¨", color: colors.green, items: ["Fastest performance", "State in memory only", "Lost on failure"] },
    { title: "Local + Fault-Tolerant", icon: "âš–ï¸", color: colors.blue, items: ["Fast memory access", "Persisted to remote storage", "Trade-off: speed vs. consistency"] },
    { title: "Remote", icon: "ðŸ”’", color: colors.purple, items: ["Native fault tolerance", "Higher latency", "Additional cost"] }
  ]}
/>

#### Example: Batch Deduplication

```python
# Using dropDuplicates in Apache Spark
dataset = session.read.schema('...').format('json').load(f'{base_dir}/input')
deduplicated = dataset.dropDuplicates(['type', 'full_name', 'version'])
```

```sql
-- Using WINDOW function
SELECT type, full_name, version FROM (
  SELECT type, full_name, version,
    ROW_NUMBER() OVER (
      PARTITION BY type, full_name, version
      ORDER BY 1
    ) AS position
  FROM duplicated_devices
) WHERE position = 1
```

#### Example: Streaming Deduplication

```python
# Define schema with event time
event_schema = StructType([
    StructField("visit_id", StringType()),
    StructField("visit_time", TimestampType())
])

# Prepare input with time column
deduplicated_visits = (input
    .select(F.from_json("value", event_schema).alias("value_struct"), "value")
    .select("value_struct.visit_time", "value_struct.visit_id", "value")
    # Configure watermark for late data boundary and state expiration
    .withWatermark("visit_time", "10 minutes")
    .dropDuplicates(["visit_id", "visit_time"])
    .drop("visit_time", "visit_id"))
```

> **Insight**
>
> The watermark has dual responsibilities: (1) defines the late data arrival boundary, and (2) controls how long the job remembers keys. Entries older than the watermark are automatically removed, preventing unbounded state growth.

#### Consequences

| Consequence | Description |
|-------------|-------------|
| **Space vs. time trade-off** | Short windows miss some duplicates; longer windows require more resources |
| **Not exactly-once delivery** | Correct deduplication doesn't guarantee exactly-once deliveryâ€”consider idempotency patterns |

---

## 4. Late Data

Late-arriving data sounds innocent but has serious implications for data pipelines. Events can be delayed due to network issues, buffering, or processing delays.

### 4.1. Pattern: Late Data Detector

**In plain English:** Like setting a deadline for party RSVPsâ€”anyone who responds after the cutoff is marked as "late," and you decide how to handle them separately.

**In technical terms:** The Late Data Detector pattern uses watermarksâ€”calculated from event time minus allowed latenessâ€”to classify incoming records as "on time" or "late" based on when events actually occurred versus when they arrive for processing.

**Why it matters:** Detecting late data is the first step to handling it properly. Without detection, late records can corrupt aggregations, break session logic, or silently disappear.

> **Insight**
>
> **Event Time vs. Processing Time:** Event time indicates when an action happened; processing time indicates when the pipeline processes it. Processing time is never lateâ€”only event time can be.

#### Problem

Most visitors generate events in near real-time (within 15 seconds). However, sometimes users lose network connectivity and buffer visits locally before flushing when connection is restored. Your jobs need to detect these late events to apply dedicated handling strategies.

#### Solution

<ProcessFlow
  direction="horizontal"
  steps={[
    { title: "Define Event Time", description: "Select time attribute to track", icon: "â°", color: colors.blue },
    { title: "Aggregate Per Partition", description: "Use MAX for monotonicity", icon: "ðŸ“Š", color: colors.purple },
    { title: "Global Aggregation", description: "MIN or MAX across partitions", icon: "ðŸŒ", color: colors.green },
    { title: "Apply Lateness", description: "MAX(event_time) - lateness", icon: "âš¡", color: colors.orange }
  ]}
/>

**Aggregation Strategies:**

<ComparisonTable
  beforeTitle="MIN Strategy"
  afterTitle="MAX Strategy"
  beforeColor={colors.blue}
  afterColor={colors.green}
  items={[
    { label: "Follows", before: "Slowest partition", after: "Fastest partition" },
    { label: "Data Coverage", before: "More data on time", after: "May skip slow partitions" },
    { label: "Buffer Size", before: "Larger buffers", after: "Smaller buffers" },
    { label: "Risk", before: "Stuck-in-the-past", after: "Aggressive dropping" }
  ]}
/>

**Watermark Calculation Example:**

| Event Times | Input Watermark | Watermark Candidate | Output Watermark | Ignored Records |
|-------------|-----------------|---------------------|------------------|-----------------|
| 10:00, 10:05, 10:06 | - | MAX(10:06) âˆ’ 30' = 9:36 | 9:36 | - |
| 9:20, 9:31, 10:07 | 9:36 | MAX(10:07) âˆ’ 30' = 9:37 | 9:37 | 9:20, 9:31 |

> **Warning**
>
> **MIN Strategy Risks:** Using MIN for partition-level event time tracking can cause "stuck-in-the-past" situations where late data repeatedly prevents the watermark from advancing, causing unbounded state growth.

#### Example: Apache Spark Structured Streaming

```python
visits_events = (input_data
    .selectExpr('CAST(value AS STRING)')
    .select(F.from_json('value',
        'visit_id INT, event_time TIMESTAMP, page STRING').alias('visit'))
    .selectExpr('visit.*'))

session_window = (visits_events
    .withWatermark('event_time', '1 hour')  # Allow 1 hour lateness
    .groupBy(F.window(F.col('event_time'), '10 minutes'))
    .count())
```

**Impact of Watermark on Processing:**

| Event Time | Watermark Transition | Buffered Windows | Emitted Windows |
|------------|---------------------|------------------|-----------------|
| 03:15 | epoch â†’ 02:15 | [03:10-03:20] | [] |
| 03:00 | 02:15 â†’ 02:15 | [03:00-03:10, 03:10-03:20] | [] |
| 01:50 | 02:15 â†’ 02:15 | [03:00-03:10, 03:10-03:20] | [] (01:50 ignored) |
| 04:31 | 02:15 â†’ 03:31 | [04:30-04:40] | [03:00-03:10, 03:10-03:20] |

#### Example: Apache Flink Late Data Capture

```python
class VisitTimestampAssigner(TimestampAssigner):
    def extract_timestamp(self, value: Any, record_timestamp: int) -> int:
        event = json.loads(value)
        event_time = datetime.datetime.fromisoformat(event['event_time'])
        return int(event_time.timestamp())

class VisitLateDataProcessor(ProcessFunction):
    def __init__(self, late_data_output: OutputTag):
        self.late_data_output = late_data_output

    def process_element(self, value: Visit, ctx: 'ProcessFunction.Context'):
        current_watermark = ctx.timer_service().current_watermark()
        if current_watermark > value.event_time:
            yield (self.late_data_output,
                   json.dumps(VisitWithStatus(visit=value, is_late=True).to_dict()))
        else:
            yield json.dumps(VisitWithStatus(visit=value, is_late=False).to_dict())
```

```python
# Configure watermark strategy
watermark_strategy = (WatermarkStrategy
    .for_bounded_out_of_orderness(Duration.of_seconds(5))
    .with_timestamp_assigner(VisitTimestampAssigner()))

# Process with late data handling
late_data_output: OutputTag = OutputTag('late_events', Types.STRING())
visits: DataStream = (data_source
    .map(map_json_to_visit)
    .process(VisitLateDataProcessor(late_data_output), Types.STRING()))

# Route to different sinks
visits.get_side_output(late_data_output).sink_to(kafka_sink_late_visits)
visits.sink_to(kafka_sink_valid_data)
```

#### Consequences

| Consequence | Description |
|-------------|-------------|
| **Framework limitations** | Some frameworks detect but don't capture late data (Spark Structured Streaming) |
| **MAX strategy skew** | In highly skewed environments, MAX-based watermarks can drop many records from slow partitions |

---

### 4.2. Pattern: Static Late Data Integrator

**In plain English:** Like checking the mailbox for the past two weeks every dayâ€”regardless of whether anything actually arrived late during that period.

**In technical terms:** The Static Late Data Integrator pattern uses a fixed lookback window to reprocess a predefined number of past partitions during each pipeline execution, ensuring late data within that window is integrated.

**Why it matters:** When late data is valuable (like e-commerce orders), losing it isn't an option. A fixed integration window provides predictable reprocessing without complex tracking infrastructure.

#### Problem

Your daily job generates statistics from referring websites. Results are approximate for 15 days (maximum allowed delay for late data). Currently, your batch only processes the current day, ignoring late data. You want to include late data integration without running 15 separate jobs daily.

#### Solution

> **Insight**
>
> **Easy for You, Hard for Others:** Using processing-time partitions moves the late data problem downstream. If your 9:00 AM partition contains 80% data for 9:00, 10% for 8:00, and 10% for 7:00, downstream event-time consumers must now handle what you avoided.

<DiagramContainer title="Static Lookback Window">
  <Row gap="md" align="center">
    <Box color={colors.slate} icon="ðŸ“…">Dec 17</Box>
    <Box color={colors.slate} icon="ðŸ“…">Dec 18</Box>
    <Box color={colors.blue} icon="ðŸ“…">...</Box>
    <Box color={colors.orange} icon="ðŸ“…">Dec 30</Box>
    <Box color={colors.green} icon="â­">Dec 31 (Current)</Box>
  </Row>
</DiagramContainer>

**Integration Strategies:**

<DiagramContainer title="Late Data Integration Placement">
  <Column gap="lg">
    <Group title="Sequential: Late Data First" color={colors.blue}>
      <Row gap="sm" align="center">
        <Box color={colors.orange} size="sm">Late Data</Box>
        <Arrow direction="right" />
        <Box color={colors.green} size="sm">Current Data</Box>
      </Row>
    </Group>
    <Group title="Parallel: Concurrent Processing" color={colors.purple}>
      <Row gap="sm" align="center">
        <Box color={colors.orange} size="sm">Late Data</Box>
        <Box color={colors.green} size="sm">Current Data</Box>
      </Row>
    </Group>
    <Group title="Sequential: Current Data First" color={colors.cyan}>
      <Row gap="sm" align="center">
        <Box color={colors.green} size="sm">Current Data</Box>
        <Arrow direction="right" />
        <Box color={colors.orange} size="sm">Late Data</Box>
      </Row>
    </Group>
  </Column>
</DiagramContainer>

**Strategy Selection:**

| Pipeline Type | Recommended Strategy |
|---------------|---------------------|
| **Stateful** | Sequential (late data first)â€”need valid history for current data |
| **Stateless** | Any strategy works; choose based on delivery priority |

#### Example: Apache Airflow Dynamic Task Mapping

```python
@task
def generate_backfilling_runs():
    dr: DagRun = get_current_context()['dag_run']
    backfilling_dates = []
    days_to_backfill = 2
    start_date_to_backfill = (dr.execution_date -
        datetime.timedelta(days=days_to_backfill))

    for days_to_add in range(0, days_to_backfill):
        date_to_backfill = start_date_to_backfill + datetime.timedelta(days=days_to_add)
        backfilling_dates.append(date_to_backfill.date().strftime('%Y-%m-%d'))
    return backfilling_dates

@task
def integrate_late_data(late_date: str):
    copy_file(late_date)

# Pipeline with dynamic task mapping
backfilling_runs_generator = generate_backfilling_runs()
(file_to_load_sensor >> load_current_file() >> backfilling_runs_generator >>
    integrate_late_data.expand(late_date=backfilling_runs_generator))
```

#### Consequences

> **Warning**
>
> **Overlapping Backfills:** With a 4-day lookback window, running executions for Oct 10-12 creates overlapping date ranges. For backfills, only restart the latest execution (Oct 12) which covers all needed dates.

| Consequence | Description |
|-------------|-------------|
| **Snowball effect** | Downstream consumers may need to replay partitions, creating cascading backfills |
| **Overlapping executions** | Consider lookback window duration before backfilling |
| **Pipeline trigger constraint** | Backfilling must be part of main pipeline, not separate pipelines |
| **Resource waste** | Fixed periods may not contain late data every time |
| **Time requirement** | Pattern requires time-based partitioning |

---

### 4.3. Pattern: Dynamic Late Data Integrator

**In plain English:** Instead of blindly checking the last two weeks of mail, you get a notification telling you exactly which days have new mail to pick up.

**In technical terms:** The Dynamic Late Data Integrator pattern uses a state table to track partition modification times, reprocessing only partitions that actually received new late data rather than a fixed lookback window.

**Why it matters:** Eliminates wasted compute on partitions without new data while extending coverage beyond fixed windows when truly needed.

#### Problem

The 15-day Static Late Data Integrator isn't enough anymore. Your product owner wants all late data integrated, even beyond 15 days. You need to adapt the pipeline to this requirement without blindly replaying two weeks of data.

#### Solution

<DiagramContainer title="State Table for Dynamic Late Data Integration">
  <Column gap="md">
    <Box color={colors.blue} icon="ðŸ“‹" size="lg">State Table</Box>
    <Row gap="lg" align="center">
      <Column gap="sm" align="center">
        <Box color={colors.slate} size="sm">Partition</Box>
        <Box color={colors.slate} size="sm">2024-12-17</Box>
        <Box color={colors.slate} size="sm">2024-12-18</Box>
      </Column>
      <Column gap="sm" align="center">
        <Box color={colors.purple} size="sm">Last Processed</Box>
        <Box color={colors.purple} size="sm">10:20</Box>
        <Box color={colors.purple} size="sm">09:55</Box>
      </Column>
      <Column gap="sm" align="center">
        <Box color={colors.orange} size="sm">Last Updated</Box>
        <Box color={colors.orange} size="sm">03:00</Box>
        <Box color={colors.orange} size="sm">10:12 (late!)</Box>
      </Column>
    </Row>
  </Column>
</DiagramContainer>

```sql
-- Query to find partitions needing backfill
SELECT partition FROM state_table
WHERE `Last update time` > `Last processed time`
  AND `Partition` < `Processed partition`
```

**Data stores with built-in partition metadata:**
- **BigQuery:** `INFORMATION_SCHEMA.PARTITIONS` with `last_modified_timestamp`
- **Apache Iceberg:** `partitions` metadata table with `last_updated_at`

> **Insight**
>
> **Entity-Level Optimization:** If you can isolate entities impacted by late data, you don't need to backfill entire partitionsâ€”just overwrite data for affected entities. This optimizes resources but is harder to implement.

#### Example: Delta Lake Version Tracking

```scala
val deltaLog = DeltaLog.forTable(sparkSession, jobArguments.tableFullPath)
val partitionsChangeVersions: Iterator[(String, Long)] = deltaLog.getChanges(0)
  .flatMap {
    case (version, actions) => {
      val changedPartitionsInVersion: Set[String] = actions.map {
        case addFile: AddFile if addFile.dataChange =>
          Some(addFile.partitionValues.map(e => s"${e._1}=${e._2}").mkString("/"))
        case removeFile: RemoveFile if removeFile.dataChange =>
          Some(removeFile.partitionValues.map(e => s"${e._1}=${e._2}").mkString("/"))
        case _ => None
      }.filter(_.isDefined).map(_.get).toSet
      changedPartitionsInVersion.map(partition => (partition, version))
    }
  }

val lastVersionForEachPartition: Map[String, Long] = partitionsChangeVersions
  .toSeq.groupBy(_._1).mapValues(_.map(_._2).max)
```

#### Handling Concurrency

For concurrent pipelines, add an `Is processed` column to prevent duplicate runs:

```sql
SELECT partition FROM state_table WHERE
  `Last update time` > `Last processed time` AND
  `Partition` < `Processed partition` AND
  `Is processed` = false
```

**Required pipeline adjustments:**

1. Start with task updating `Is processed` for current partition (depends on previous run)
2. Task generating partitions also marks them as processed (depends on previous run)
3. Task updating `last processed time` resets `Is processed` to false

```python
# Apache Airflow with sequential dependency
with DAG('devices_loader', max_active_runs=5,
    default_args={'depends_on_past': False},
) as dag:
    processing_marker = SparkKubernetesOperator(
        task_id='mark_partition_as_being_processed',
        depends_on_past=True  # Sequential enforcement
    )
    backfill_creation_job = SparkKubernetesOperator(
        task_id='get_late_partitions_and_mark_them_as_being_processed',
        depends_on_past=True  # Sequential enforcement
    )
```

#### Consequences

> **Warning**
>
> **Stateful Pipelines and Very Late Data:** If your stateful pipeline's last run was Oct 20 and you receive late data for Sep 21, you must regenerate all executions from Sep 21 to Oct 20 to maintain correctness. Consider an accepted lookback limit even for dynamic windows.

| Consequence | Description |
|-------------|-------------|
| **Concurrency risks** | May generate duplicate runs without proper `Is processed` tracking |
| **Stateful pipeline cascade** | Very late data can trigger massive backfills for stateful jobs |
| **Scheduling complexity** | Requires tracking infrastructure and partition metadata access |

---

## 5. Filtering

Data engineering errors aren't always technical failuresâ€”human mistakes like incorrect filter implementations can expose partial or bad data to end users.

### 5.1. Pattern: Filter Interceptor

**In plain English:** Like a detailed receipt showing exactly which items were rejected at airport security and whyâ€”instead of just knowing "some items were confiscated."

**In technical terms:** The Filter Interceptor pattern wraps filtering conditions with counters to track exactly how many records each filter eliminates, enabling detection of aggressive or buggy filtering logic.

**Why it matters:** When filter-induced data volume suddenly spikes from 15% to 90%, you need to know if it's a data issue or a software regressionâ€”and which specific filter caused it.

#### Problem

Your batch job recently showed a sudden spike of filtered data from 15% to 90%. You can't determine if this is a data issue or software regression because the framework optimizes multiple filters into a single execution step, showing only total filtered rows.

#### Solution

<DiagramContainer title="Filter Interceptor Pattern">
  <Row gap="md" align="center">
    <Box color={colors.blue} icon="ðŸ“¥">Input Record</Box>
    <Arrow direction="right" />
    <Column gap="sm">
      <Box color={colors.purple} icon="ðŸ”" size="sm">Filter 1 + Counter</Box>
      <Box color={colors.purple} icon="ðŸ”" size="sm">Filter 2 + Counter</Box>
      <Box color={colors.purple} icon="ðŸ”" size="sm">Filter N + Counter</Box>
    </Column>
    <Arrow direction="right" />
    <Box color={colors.green} icon="ðŸ“¤">Valid / Filtered</Box>
  </Row>
</DiagramContainer>

**Implementation approaches:**

| API Type | Implementation |
|----------|----------------|
| **Programmatic** | Wrap filters with counters, increment on true evaluation |
| **SQL/Declarative** | Use subquery exposing filter conditions as columns |

> **Insight**
>
> **Right Tool for the Job:** If programmatic API is better for your needs, use itâ€”even if you've been writing SQL exclusively. The Filter Interceptor is a case where declarative languages are less powerful.

#### Example: PySpark with Accumulators

```python
@dataclasses.dataclass
class FilterWithAccumulator:
    name: str
    filter: Callable[[Any], bool]
    accumulator: Accumulator[int]

filters_with_accumulators = {
    'type': [
        FilterWithAccumulator(
            'type is null',
            lambda device: device['type'] is not None,
            spark_context.accumulator(0)
        ),
        FilterWithAccumulator(
            'type is too short (1 char or less)',
            lambda device: len(device['type']) > 1,
            spark_context.accumulator(0)
        )
    ],
    # ... more filters
}

def filter_null_type(devices_iterator: Iterator[pandas.DataFrame]):
    def filter_row_with_accumulator(device_row):
        for device_row_attribute in device_row.keys():
            for filter_with_acc in filters_with_accumulators[device_row_attribute]:
                if not filter_with_acc.filter(device_row):
                    filter_with_acc.accumulator.add(1)
                    return False
        return True

    for devices_df in devices_iterator:
        yield devices_df[devices_df.apply(
            lambda device: filter_row_with_accumulator(device), axis=1
        ) == True]

valid_devices = input_dataset.mapInPandas(filter_null_type, input_dataset.schema)
valid_devices.write.mode('append').format('delta').save(output_dir)

# Get filtering statistics
for key, accumulators in filters_with_accumulators.items():
    for acc in accumulators:
        print(f'{key} // {acc.name} // {acc.accumulator.value}')
```

#### Example: SQL Implementation

```sql
-- Create intermediate table with filter flags
SELECT * FROM (
  SELECT
    CASE
      WHEN (type IS NOT NULL) IS FALSE THEN 'null_type'
      WHEN (LEN(type) > 2) IS FALSE THEN 'short_type'
      WHEN (full_name IS NOT NULL) IS FALSE THEN 'null_full_name'
      WHEN (version IS NOT NULL) IS FALSE THEN 'null_version'
      ELSE NULL
    END AS status_flag,
    type, full_name, version
  FROM input
);

-- Get filter statistics
SELECT COUNT(*), status_flag
FROM input_with_flags
WHERE status_flag IS NOT NULL
GROUP BY status_flag;

-- Write valid records
SELECT type, full_name, version
FROM input_with_flags
WHERE status_flag IS NULL;
```

#### Consequences

| Consequence | Description |
|-------------|-------------|
| **Runtime impact** | Counter overhead is small; SQL may need temporary tables |
| **Declarative complexity** | SQL implementation is more verbose and harder to maintain |
| **Streaming challenges** | May require stateful transformation with time boundaries for statistics |

---

## 6. Fault Tolerance

Continuous data processing workflows like streaming need mechanisms to track progress and recover from failures without reprocessing all data.

### 6.1. Pattern: Checkpointer

**In plain English:** Like a video game autosaveâ€”when you crash, you don't start from the very beginning; you resume from the last saved checkpoint.

**In technical terms:** The Checkpointer pattern persists processing progress (positions in data sources and computed state) to durable storage, enabling recovery from failures without complete reprocessing.

**Why it matters:** Streaming applications work on continuously arriving events in append-only logs. Without checkpointing, any restart would reprocess all historical dataâ€”potentially days or weeks worth.

#### Problem

Your streaming job counts unique visits in 10-minute windows. You're worried that fatal failures will stop the job and force it to reprocess from the beginning. You need a solution that persists results as the query progresses.

#### Solution

<DiagramContainer title="Checkpointing Approaches">
  <Row gap="lg" align="center">
    <Group title="Framework-Based" color={colors.blue}>
      <Column gap="sm">
        <Box color={colors.blue} size="sm">Spark Structured Streaming</Box>
        <Box color={colors.blue} size="sm">Apache Flink</Box>
        <Box color={colors.slate} size="sm" icon="ðŸ’¾">Object Store</Box>
      </Column>
    </Group>
    <Group title="Data Store-Based" color={colors.green}>
      <Column gap="sm">
        <Box color={colors.green} size="sm">Kafka SDK</Box>
        <Box color={colors.green} size="sm">Kinesis KCL</Box>
        <Box color={colors.slate} size="sm" icon="ðŸ’¾">Kafka Topic / DynamoDB</Box>
      </Column>
    </Group>
  </Row>
</DiagramContainer>

**Checkpoint implementations:**

| Type | Description |
|------|-------------|
| **Configuration-driven** | Configure frequency, delegate execution to framework (Spark, Flink) |
| **Intentional action** | Explicitly call commit methods after processing (Kafka SDK) |

**Delivery Modes:**

<CardGrid
  columns={3}
  cards={[
    { title: "At Most Once", icon: "1ï¸âƒ£", color: colors.orange, items: ["Checkpoint before processing", "May lose data on failure"] },
    { title: "At Least Once", icon: "âž•", color: colors.blue, items: ["Checkpoint after processing", "May create duplicates on retry"] },
    { title: "Exactly Once", icon: "âœ“", color: colors.green, items: ["Requires idempotency patterns", "Checkpointing alone insufficient"] }
  ]}
/>

#### Example: Apache Spark Structured Streaming

```python
write_query = (input_stream_data
    .writeStream
    .outputMode('update')
    .option('checkpointLocation', f'{base_dir}/checkpoint')
    .foreachBatch(synchronize_visits_to_files)
    .start())
```

Checkpoint metadata file example:
```json
// /tmp/checkpoint/offsets/18
{"visits":{"1":1276,"0":1224}}
```

#### Example: Apache Flink Time-Based Checkpointing

```python
checkpoint_interval_30_sec = 30000
env.enable_checkpointing(checkpoint_interval_30_sec, mode=EXACTLY_ONCE)

# Retain checkpoints after job cancellation for recovery
env.get_checkpoint_config().enable_externalized_checkpoints(RETAIN_ON_CANCELLATION)
```

- **`RETAIN_ON_CANCELLATION`**: Keep checkpoint files after job failure (normally removed on restart)
- **`EXACTLY_ONCE` mode**: Guarantees each input record reflects in state exactly once

> **Insight**
>
> **Spark vs. Flink Checkpointing:** Spark checkpoints at every iteration (stronger guarantee, more overhead). Flink checkpoints at time intervals (configurable trade-off between latency and recovery time).

#### Consequences

> **Warning**
>
> **Not True Exactly-Once:** Checkpointing gives an exactly-once "feeling" but distributed jobs with parallel tasks can still reprocess records on partial failures. True exactly-once delivery requires idempotency patterns from Chapter 4.

| Consequence | Description |
|-------------|-------------|
| **Delivery vs. latency trade-off** | More frequent checkpoints = slower job but faster recovery |
| **State overhead** | Stateful applications (like sessionizers) have larger state to checkpoint |
| **Distributed failures** | Parallel task failures can still cause reprocessing despite checkpoints |

---

## 7. Summary

### Key Takeaways

<CardGrid
  columns={2}
  cards={[
    { title: "Dead-Letter", icon: "ðŸ“¬", color: colors.red, items: ["Isolate unprocessable records", "Keep pipeline running", "Enable post-mortem analysis"] },
    { title: "Windowed Deduplicator", icon: "ðŸ”‘", color: colors.blue, items: ["Eliminate duplicates in scope", "Batch: full dataset", "Streaming: time windows"] },
    { title: "Late Data Detector", icon: "â°", color: colors.orange, items: ["Watermark-based detection", "Event time vs. processing time", "Monotonically increasing"] },
    { title: "Late Data Integrators", icon: "ðŸ”„", color: colors.purple, items: ["Static: fixed lookback window", "Dynamic: only changed partitions", "Watch for snowball effects"] },
    { title: "Filter Interceptor", icon: "ðŸ”", color: colors.green, items: ["Track filter statistics", "Detect aggressive filtering", "Debug data volume changes"] },
    { title: "Checkpointer", icon: "ðŸ’¾", color: colors.cyan, items: ["Persist progress to storage", "Enable failure recovery", "Not true exactly-once alone"] }
  ]}
/>

### Pattern Selection Guide

| Problem | Pattern |
|---------|---------|
| Bad records crashing pipeline | Dead-Letter |
| Duplicate records in output | Windowed Deduplicator |
| Need to identify late arrivals | Late Data Detector |
| Fixed window late data integration | Static Late Data Integrator |
| Variable window late data integration | Dynamic Late Data Integrator |
| Unknown filter impact | Filter Interceptor |
| Streaming job recovery | Checkpointer |

> **Insight**
>
> Error management doesn't guarantee perfectly valid data. Even with these patterns giving an "exactly-once" feeling, retries and backfills can still impact data quality. The next chapter on Idempotency patterns addresses this challenge.

---

**Previous:** [Chapter 2: Data Ingestion Design Patterns](./chapter2) | **Next:** [Chapter 4: Idempotency Design Patterns](./chapter4)
